{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prerequisites\n",
    "# pip install spicy\n",
    "# pip install scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas==0.25.3"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: After October 2020 you may experience errors when installing or updating packages. This is because pip will change the way that it resolves dependency conflicts.\n",
      "\n",
      "We recommend you use --use-feature=2020-resolver to test your packages with the new resolver before it becomes the default.\n",
      "\n",
      "woodwork 0.4.2 requires pandas<1.2.5,>=1.2.0, but you'll have pandas 0.25.3 which is incompatible.\n",
      "sktime 0.7.0 requires numpy>=1.19.0, but you'll have numpy 1.18.5 which is incompatible.\n",
      "sktime 0.7.0 requires pandas>=1.1.0, but you'll have pandas 0.25.3 which is incompatible.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Using cached pandas-0.25.3-cp38-cp38-win_amd64.whl (9.4 MB)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in c:\\users\\sasmitabhoi\\anaconda3\\lib\\site-packages (from pandas==0.25.3) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in c:\\users\\sasmitabhoi\\appdata\\roaming\\python\\python38\\site-packages (from pandas==0.25.3) (2020.5)\n",
      "Requirement already satisfied: numpy>=1.13.3 in c:\\users\\sasmitabhoi\\appdata\\roaming\\python\\python38\\site-packages (from pandas==0.25.3) (1.18.5)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\sasmitabhoi\\anaconda3\\lib\\site-packages (from python-dateutil>=2.6.1->pandas==0.25.3) (1.15.0)\n",
      "Installing collected packages: pandas\n",
      "  Attempting uninstall: pandas\n",
      "    Found existing installation: pandas 1.1.5\n",
      "    Uninstalling pandas-1.1.5:\n",
      "      Successfully uninstalled pandas-1.1.5\n",
      "Successfully installed pandas-1.2.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sktime 0.7.0 requires scikit-learn>=0.24.0, but you'll have scikit-learn 0.23.2 which is incompatible.\n",
      "pmdarima 1.8.0 requires Cython<0.29.18,>=0.29, but you'll have cython 0.29.23 which is incompatible.\n",
      "ktrain 0.25.4 requires pandas>=1.0.1, but you'll have pandas 0.25.3 which is incompatible.\n",
      "featuretools 0.25.0 requires pandas<2.0.0,>=1.2.0, but you'll have pandas 0.25.3 which is incompatible.\n",
      "evalml 0.28.0 requires numpy>=1.20.0, but you'll have numpy 1.18.5 which is incompatible.\n",
      "evalml 0.28.0 requires pandas>=1.2.1, but you'll have pandas 0.25.3 which is incompatible.\n",
      "evalml 0.28.0 requires scikit-learn>=0.24.0, but you'll have scikit-learn 0.23.2 which is incompatible.\n",
      "vaex-core 4.1.0 requires pyarrow>=3.0, but you'll have pyarrow 1.0.0 which is incompatible.\n",
      "simpletransformers 0.60.9 requires transformers>=4.2.0, but you'll have transformers 3.5.1 which is incompatible.\n",
      "ppscore 1.2.0 requires pandas<2.0.0,>=1.0.0, but you'll have pandas 0.25.3 which is incompatible.\n",
      "phik 0.11.2 requires scipy>=1.5.2, but you'll have scipy 1.4.1 which is incompatible.\n",
      "pandas-visual-analysis 0.0.4 requires numpy>=1.19.1, but you'll have numpy 1.18.5 which is incompatible.\n",
      "pandas-visual-analysis 0.0.4 requires pandas>=1.1.0, but you'll have pandas 0.25.3 which is incompatible.\n",
      "modin 0.9.1 requires pandas==1.2.3, but you'll have pandas 0.25.3 which is incompatible.\n",
      "dataprep 0.2.15 requires dask[array,dataframe,delayed]<3.0,>=2.25, but you'll have dask 2021.4.0 which is incompatible.\n",
      "dataprep 0.2.15 requires pandas<2,>=1, but you'll have pandas 0.25.3 which is incompatible.\n",
      "ERROR: Could not install packages due to an EnvironmentError: [WinError 5] Access is denied: 'C:\\\\Users\\\\Sasmitabhoi\\\\AppData\\\\Roaming\\\\Python\\\\Python38\\\\site-packages\\\\~umpy\\\\.libs\\\\libopenblas.PYQHXLVVQ7VESDPUVUADXEVJOBGHJPAY.gfortran-win_amd64.dll'\n",
      "Consider using the `--user` option or check the permissions.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting numpy==1.17.3\n",
      "  Using cached numpy-1.17.3-cp38-cp38-win_amd64.whl (12.7 MB)\n",
      "Installing collected packages: numpy\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.18.5\n",
      "    Uninstalling numpy-1.18.5:\n",
      "      Successfully uninstalled numpy-1.18.5\n",
      "Collecting Keras==2.3.1"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: After October 2020 you may experience errors when installing or updating packages. This is because pip will change the way that it resolves dependency conflicts.\n",
      "\n",
      "We recommend you use --use-feature=2020-resolver to test your packages with the new resolver before it becomes the default.\n",
      "\n",
      "keras-bert 0.86.0 requires Keras>=2.4.3, but you'll have keras 2.3.1 which is incompatible.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Using cached Keras-2.3.1-py2.py3-none-any.whl (377 kB)\n",
      "Requirement already satisfied: six>=1.9.0 in c:\\users\\sasmitabhoi\\anaconda3\\lib\\site-packages (from Keras==2.3.1) (1.15.0)\n",
      "Requirement already satisfied: numpy>=1.9.1 in c:\\users\\sasmitabhoi\\anaconda3\\lib\\site-packages (from Keras==2.3.1) (1.18.2)\n",
      "Requirement already satisfied: h5py in c:\\users\\sasmitabhoi\\anaconda3\\lib\\site-packages (from Keras==2.3.1) (2.10.0)\n",
      "Requirement already satisfied: keras-applications>=1.0.6 in c:\\users\\sasmitabhoi\\anaconda3\\lib\\site-packages (from Keras==2.3.1) (1.0.8)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in c:\\users\\sasmitabhoi\\appdata\\roaming\\python\\python38\\site-packages (from Keras==2.3.1) (1.1.2)\n",
      "Requirement already satisfied: scipy>=0.14 in c:\\users\\sasmitabhoi\\anaconda3\\lib\\site-packages (from Keras==2.3.1) (1.4.1)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\sasmitabhoi\\appdata\\roaming\\python\\python38\\site-packages (from Keras==2.3.1) (5.4.1)\n",
      "Installing collected packages: Keras\n",
      "  Attempting uninstall: Keras\n",
      "    Found existing installation: Keras 2.4.3\n",
      "    Uninstalling Keras-2.4.3:\n",
      "      Successfully uninstalled Keras-2.4.3\n",
      "Successfully installed Keras-2.3.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "simpletransformers 0.60.9 requires transformers>=4.2.0, but you'll have transformers 3.5.1 which is incompatible.\n",
      "ERROR: Could not find a version that satisfies the requirement tensorflow==2.0.0 (from versions: 2.2.0, 2.2.1, 2.2.2, 2.2.3, 2.3.0, 2.3.1, 2.3.2, 2.3.3, 2.3.4, 2.4.0, 2.4.1, 2.4.2, 2.4.3, 2.4.4, 2.5.0, 2.5.1, 2.5.2, 2.5.3, 2.6.0rc0, 2.6.0rc1, 2.6.0rc2, 2.6.0, 2.6.1, 2.6.2, 2.6.3, 2.6.4, 2.6.5, 2.7.0rc0, 2.7.0rc1, 2.7.0, 2.7.1, 2.7.2, 2.7.3, 2.8.0rc0, 2.8.0rc1, 2.8.0, 2.8.1, 2.8.2, 2.9.0rc0, 2.9.0rc1, 2.9.0rc2, 2.9.0, 2.9.1)\n",
      "ERROR: No matching distribution found for tensorflow==2.0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tqdm==4.43.0"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: After October 2020 you may experience errors when installing or updating packages. This is because pip will change the way that it resolves dependency conflicts.\n",
      "\n",
      "We recommend you use --use-feature=2020-resolver to test your packages with the new resolver before it becomes the default.\n",
      "\n",
      "evalml 0.28.0 requires numpy>=1.20.0, but you'll have numpy 1.18.2 which is incompatible.\n",
      "evalml 0.28.0 requires scikit-learn>=0.24.0, but you'll have scikit-learn 0.23.2 which is incompatible.\n",
      "simpletransformers 0.60.9 requires tqdm>=4.47.0, but you'll have tqdm 4.43.0 which is incompatible.\n",
      "simpletransformers 0.60.9 requires transformers>=4.2.0, but you'll have transformers 3.5.1 which is incompatible.\n",
      "pandas-profiling 2.11.0 requires tqdm>=4.48.2, but you'll have tqdm 4.43.0 which is incompatible.\n",
      "dataprep 0.2.15 requires dask[array,dataframe,delayed]<3.0,>=2.25, but you'll have dask 2021.4.0 which is incompatible.\n",
      "dataprep 0.2.15 requires tqdm<5.0,>=4.48, but you'll have tqdm 4.43.0 which is incompatible.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Using cached tqdm-4.43.0-py2.py3-none-any.whl (59 kB)\n",
      "Installing collected packages: tqdm\n",
      "  Attempting uninstall: tqdm\n",
      "    Found existing installation: tqdm 4.56.0\n",
      "    Uninstalling tqdm-4.56.0:\n",
      "      Successfully uninstalled tqdm-4.56.0\n",
      "Successfully installed tqdm-4.62.3\n",
      "Collecting matplotlib==3.1.3\n",
      "  Using cached matplotlib-3.1.3-cp38-cp38-win_amd64.whl (9.1 MB)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\sasmitabhoi\\appdata\\roaming\\python\\python38\\site-packages (from matplotlib==3.1.3) (1.3.1)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in c:\\users\\sasmitabhoi\\anaconda3\\lib\\site-packages (from matplotlib==3.1.3) (2.8.1)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in c:\\users\\sasmitabhoi\\anaconda3\\lib\\site-packages (from matplotlib==3.1.3) (2.4.7)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\sasmitabhoi\\anaconda3\\lib\\site-packages (from matplotlib==3.1.3) (0.10.0)\n",
      "Requirement already satisfied: numpy>=1.11 in c:\\users\\sasmitabhoi\\anaconda3\\lib\\site-packages (from matplotlib==3.1.3) (1.18.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\sasmitabhoi\\anaconda3\\lib\\site-packages (from python-dateutil>=2.1->matplotlib==3.1.3) (1.15.0)\n",
      "Installing collected packages: matplotlib\n",
      "  Attempting uninstall: matplotlib\n",
      "    Found existing installation: matplotlib 3.3.4\n",
      "    Uninstalling matplotlib-3.3.4:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not install packages due to an EnvironmentError: [WinError 5] Access is denied: 'c:\\\\users\\\\sasmitabhoi\\\\appdata\\\\roaming\\\\python\\\\python38\\\\site-packages\\\\matplotlib\\\\backends\\\\_backend_agg.cp38-win_amd64.pyd'\n",
      "Consider using the `--user` option or check the permissions.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas==0.25.3\n",
    "!pip install numpy==1.17.3\n",
    "!pip install Keras==2.3.1\n",
    "!pip install tensorflow==2.0.0\n",
    "!pip install tqdm==4.43.0\n",
    "!pip install matplotlib==3.1.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "\n",
    "def create_unique_word_dict(text:list) -> dict:\n",
    "    \"\"\"\n",
    "    A method that creates a dictionary where the keys are unique words \n",
    "    and key values are indices .\n",
    "    \"\"\" \n",
    "    # Getting all the unique words from our text and sorting them alphabetically\n",
    "    words = list(set(text))\n",
    "    words.sort()\n",
    "    \n",
    "    # Creating the dictionary for the unique words\n",
    "    unique_word_dict = {}\n",
    "    for i,word in enumerate(words):\n",
    "        unique_word_dict.update({\n",
    "            word: i\n",
    "        })\n",
    "    \n",
    "    print(unique_word_dict)\n",
    "    return unique_word_dict\n",
    "\n",
    "def text_preprocessing(\n",
    "    text:list,\n",
    "    punctuations = r'''!()-[]{};:'\"\\,<>./?@#$%^&*_â€œ~''',\n",
    "    stop_words=['and','a','is','the','in','be','will']\n",
    "    )->list:\n",
    "    \"\"\"\n",
    "    A method to preprocess text\n",
    "    \"\"\"\n",
    "    for x  in text.lower():\n",
    "        if x in punctuations:\n",
    "            text = text.replace(x,\"\")\n",
    "            \n",
    "    # Removing words that have numbers in them\n",
    "    text = re.sub(r'\\w*\\d\\w*','',text)\n",
    "    \n",
    "    # Removing digits\n",
    "    text = re.sub(r'[0-9]+','',text)\n",
    "    \n",
    "    # Cleaning the white spaces\n",
    "    text = re.sub(r'\\s+',' ',text).strip()\n",
    "    \n",
    "    # Setting every word to lower\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Converting all our text to a list\n",
    "    text = text.split(' ')\n",
    "    \n",
    "    # Droping empty strings\n",
    "    text = [x for x in text if x!='']\n",
    "    \n",
    "    # Droping stopwords\n",
    "    text = [x for  x in text if x not in stop_words]\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Functions to find the most similar words\n",
    "\n",
    "def euclidean(vec1:np.array, vec2:np.array) -> float:\n",
    "    \"\"\"\n",
    "    A function to calculate the euclidean distance between two vectors\n",
    "    \"\"\"\n",
    "    return np.sqrt(np.sum((vec1 - vec2)**2))\n",
    "\n",
    "def find_similar(word:str, embedding_dict:dict, top_n=10)->list:\n",
    "    \"\"\"\n",
    "    A method to find the most similar word based on the learnt embeddings\n",
    "    \"\"\"\n",
    "    dist_dict = {}\n",
    "    word_vector = embedding_dict.get(word, [])\n",
    "    if len(word_vector) > 0:\n",
    "        for key,value in embedding_dict.items():\n",
    "            if key!=word:\n",
    "                dist = euclidean(word_vector,value)\n",
    "                dist_dict.update({\n",
    "                    key: dist\n",
    "                })\n",
    "     \n",
    "    return sorted(dist_dict.items(),key=lambda x: x[1])[0:top_n]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Drawing the embeddings\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Deep Learning:\n",
    "from keras.models import Input,Model\n",
    "from keras.layers import Dense\n",
    "\n",
    "#from spicy import sparse # Change\n",
    "from scipy  import sparse # Change\n",
    "\n",
    "# Custom functions\n",
    "# from utility import text_preprocessing , create_unique_word_dict\n",
    "\n",
    "# Reading the text from the input folder\n",
    "texts = pd.read_csv('input/sample.csv')\n",
    "texts = [x for x in texts[\"*text\"]] # Changed\n",
    "\n",
    "# Defining the window for context\n",
    "window = 2\n",
    "\n",
    "# Creating a placeholder for the  scanning of the  word list\n",
    "word_lists = []\n",
    "all_text = []\n",
    "\n",
    "for text in texts:\n",
    "    \n",
    "    # Cleaning the text\n",
    "    text = text_preprocessing(text)\n",
    "    print(text)\n",
    "    # Appending to all the text list\n",
    "    all_text += text\n",
    "    \n",
    "    # Creating a context dictionary\n",
    "    for i, word in enumerate(text):\n",
    "        print(\"......i.......\",i,\"......word.....\",word)\n",
    "        for w in range(3):\n",
    "            # Getting the context that  is ahead by *window* words\n",
    "            if  i + 1 + w < len(text):\n",
    "                word_lists.append([word] + [text[(i + 1 + w)]])\n",
    "            # Getting the context that is behind by *window* words\n",
    "            if i - w - 1 >= 0:\n",
    "                word_lists.append([word] + [text[(i - w - 1)]])\n",
    "                \n",
    "print(\"word list \",word_lists)                \n",
    "unique_word_dict = create_unique_word_dict(all_text)\n",
    "print(\"unique_word_dict\",unique_word_dict)\n",
    "\n",
    "# Defining the number of features (unique words)\n",
    "n_words = len(unique_word_dict)\n",
    "\n",
    "# Getting all the unique words\n",
    "words = list(unique_word_dict.keys())\n",
    "\n",
    "print(\"words\",words)\n",
    "\n",
    "# Creating the X  & Y  metrics using one hot encoding\n",
    "X = []\n",
    "Y = []\n",
    "\n",
    "for i , word_list in tqdm(enumerate(word_lists)):\n",
    "    # Getting the indices\n",
    "    \n",
    "    main_word_index = unique_word_dict.get(word_list[0])\n",
    "    context_word_index = unique_word_dict.get(word_list[1])\n",
    "    print(\"......i......\",i,\".......word_list.......\",word_list,\"word_list[0]\",word_list[0],\"word_list[1]\",word_list[1])\n",
    "    #Creating the placeholders\n",
    "    X_row = np.zeros(n_words)\n",
    "    Y_row = np.zeros(n_words)\n",
    "    \n",
    "    #One hot encoding the main word\n",
    "    X_row[main_word_index] = 1\n",
    "    \n",
    "    # One hot encoding the Y matrix words\n",
    "    Y_row[context_word_index] = 1\n",
    "    \n",
    "    # Appending to the main matrices\n",
    "    X.append(X_row)\n",
    "    Y.append(Y_row)\n",
    "    \n",
    "# Converting the matrices into  a sparse format because the vast majority of the data are 0s\n",
    "X = sparse.csr_matrix(X)\n",
    "Y = sparse.csr_matrix(Y)\n",
    "\n",
    "# Defining the size of  the embedding\n",
    "embed_size = 2\n",
    "\n",
    "# Defining  the neural network\n",
    "inp = Input(shape=(X.shape[1],))\n",
    "x = Dense(units=embed_size,activation='linear')(inp)\n",
    "x = Dense(units=Y.shape[1],activation='softmax')(x)\n",
    "model = Model(inputs=inp, outputs=x)\n",
    "model.compile(loss = 'categorical_crossentropy',optimizer = 'adam')\n",
    "\n",
    "# Optimizing the network weights\n",
    "model.fit(\n",
    "    x=X,\n",
    "    y=Y,\n",
    "    batch_size=256,\n",
    "    epochs=1000\n",
    "    )\n",
    "\n",
    "# Obtaining the weights from the neural network\n",
    "# These are the so called word embeddings\n",
    "\n",
    "# The input layer\n",
    "weights = model.get_weights()[0]\n",
    "\n",
    "# Creating a dictionary to store the embeddings in . They key is a unique word and  \n",
    "# the value is the numeric vector\n",
    "embedding_dict = {}\n",
    "for word in words:\n",
    "    embedding_dict.update({\n",
    "        word: weights[unique_word_dict.get(word)]\n",
    "    })\n",
    "\n",
    "# Plotting the embeddings\n",
    "plt.figure(figsize=(10,10))\n",
    "for word in list(unique_word_dict.keys()):\n",
    "    coord = embedding_dict.get(word)\n",
    "    plt.scatter(coord[0],coord[1])\n",
    "    plt.annotate(word,(coord[0],coord[1]))\n",
    "    \n",
    "\n",
    "# Saving the embedding vector to a txt file\n",
    "try:\n",
    "    os.mkdir(f'{os.getcwd()}\\\\output')\n",
    "except Exception as e:\n",
    "    print(f'Cannot create output folder:{e}')\n",
    "\n",
    "with open(f'{os.getcwd()}\\\\output\\\\embedding.txt','w') as f:\n",
    "    for key, value  in embedding_dict.items():\n",
    "        try:\n",
    "            f.write(f'{key}: {value}\\n')\n",
    "        except Exception as e:\n",
    "            print(f'Cannot write word {key} to dict: {e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The future king is the prince',\n",
       " 'Daughter is the princess',\n",
       " 'Son is the prince',\n",
       " 'Only a man can be a king',\n",
       " 'Only a woman can be a queen',\n",
       " 'The princess will be a queen',\n",
       " 'Queen and king rule the realm',\n",
       " 'The prince is a strong man',\n",
       " 'The princess is a beautiful  woman',\n",
       " 'The  royal family is the king and queen and their children',\n",
       " 'Prince is only a boy now',\n",
       " 'A boy will be  a man']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "texts = pd.read_csv('input/sample.csv')\n",
    "texts = [x for x in texts[\"*text\"]]\n",
    "texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text_preprocessing :  ['future', 'king', 'prince']\n",
      "......i....... 0 ......word..... future\n",
      "......i....... 1 ......word..... king\n",
      "......i....... 2 ......word..... prince\n",
      "text_preprocessing :  ['daughter', 'princess']\n",
      "......i....... 0 ......word..... daughter\n",
      "......i....... 1 ......word..... princess\n",
      "text_preprocessing :  ['son', 'prince']\n",
      "......i....... 0 ......word..... son\n",
      "......i....... 1 ......word..... prince\n",
      "text_preprocessing :  ['only', 'man', 'can', 'king']\n",
      "......i....... 0 ......word..... only\n",
      "......i....... 1 ......word..... man\n",
      "......i....... 2 ......word..... can\n",
      "......i....... 3 ......word..... king\n",
      "text_preprocessing :  ['only', 'woman', 'can', 'queen']\n",
      "......i....... 0 ......word..... only\n",
      "......i....... 1 ......word..... woman\n",
      "......i....... 2 ......word..... can\n",
      "......i....... 3 ......word..... queen\n",
      "text_preprocessing :  ['princess', 'queen']\n",
      "......i....... 0 ......word..... princess\n",
      "......i....... 1 ......word..... queen\n",
      "text_preprocessing :  ['queen', 'king', 'rule', 'realm']\n",
      "......i....... 0 ......word..... queen\n",
      "......i....... 1 ......word..... king\n",
      "......i....... 2 ......word..... rule\n",
      "......i....... 3 ......word..... realm\n",
      "text_preprocessing :  ['prince', 'strong', 'man']\n",
      "......i....... 0 ......word..... prince\n",
      "......i....... 1 ......word..... strong\n",
      "......i....... 2 ......word..... man\n",
      "text_preprocessing :  ['princess', 'beautiful', 'woman']\n",
      "......i....... 0 ......word..... princess\n",
      "......i....... 1 ......word..... beautiful\n",
      "......i....... 2 ......word..... woman\n",
      "text_preprocessing :  ['royal', 'family', 'king', 'queen', 'their', 'children']\n",
      "......i....... 0 ......word..... royal\n",
      "......i....... 1 ......word..... family\n",
      "......i....... 2 ......word..... king\n",
      "......i....... 3 ......word..... queen\n",
      "......i....... 4 ......word..... their\n",
      "......i....... 5 ......word..... children\n",
      "text_preprocessing :  ['prince', 'only', 'boy', 'now']\n",
      "......i....... 0 ......word..... prince\n",
      "......i....... 1 ......word..... only\n",
      "......i....... 2 ......word..... boy\n",
      "......i....... 3 ......word..... now\n",
      "text_preprocessing :  ['boy', 'man']\n",
      "......i....... 0 ......word..... boy\n",
      "......i....... 1 ......word..... man\n",
      "word list :::::::::::::::  [['future', 'king'], ['future', 'prince'], ['king', 'prince'], ['king', 'future'], ['prince', 'king'], ['prince', 'future'], ['daughter', 'princess'], ['princess', 'daughter'], ['son', 'prince'], ['prince', 'son'], ['only', 'man'], ['only', 'can'], ['only', 'king'], ['man', 'can'], ['man', 'only'], ['man', 'king'], ['can', 'king'], ['can', 'man'], ['can', 'only'], ['king', 'can'], ['king', 'man'], ['king', 'only'], ['only', 'woman'], ['only', 'can'], ['only', 'queen'], ['woman', 'can'], ['woman', 'only'], ['woman', 'queen'], ['can', 'queen'], ['can', 'woman'], ['can', 'only'], ['queen', 'can'], ['queen', 'woman'], ['queen', 'only'], ['princess', 'queen'], ['queen', 'princess'], ['queen', 'king'], ['queen', 'rule'], ['queen', 'realm'], ['king', 'rule'], ['king', 'queen'], ['king', 'realm'], ['rule', 'realm'], ['rule', 'king'], ['rule', 'queen'], ['realm', 'rule'], ['realm', 'king'], ['realm', 'queen'], ['prince', 'strong'], ['prince', 'man'], ['strong', 'man'], ['strong', 'prince'], ['man', 'strong'], ['man', 'prince'], ['princess', 'beautiful'], ['princess', 'woman'], ['beautiful', 'woman'], ['beautiful', 'princess'], ['woman', 'beautiful'], ['woman', 'princess'], ['royal', 'family'], ['royal', 'king'], ['royal', 'queen'], ['family', 'king'], ['family', 'royal'], ['family', 'queen'], ['family', 'their'], ['king', 'queen'], ['king', 'family'], ['king', 'their'], ['king', 'royal'], ['king', 'children'], ['queen', 'their'], ['queen', 'king'], ['queen', 'children'], ['queen', 'family'], ['queen', 'royal'], ['their', 'children'], ['their', 'queen'], ['their', 'king'], ['their', 'family'], ['children', 'their'], ['children', 'queen'], ['children', 'king'], ['prince', 'only'], ['prince', 'boy'], ['prince', 'now'], ['only', 'boy'], ['only', 'prince'], ['only', 'now'], ['boy', 'now'], ['boy', 'only'], ['boy', 'prince'], ['now', 'boy'], ['now', 'only'], ['now', 'prince'], ['boy', 'man'], ['man', 'boy']]\n"
     ]
    }
   ],
   "source": [
    "window = 2\n",
    "\n",
    "# Creating a placeholder for the  scanning of the  word list\n",
    "word_lists = []\n",
    "all_text = []\n",
    "\n",
    "for text in texts:\n",
    "    \n",
    "    # Cleaning the text\n",
    "    text = text_preprocessing(text)\n",
    "    print(\"text_preprocessing : \",text)\n",
    "    # Appending to all the text list\n",
    "    all_text += text\n",
    "    \n",
    "    # Creating a context dictionary\n",
    "    for i, word in enumerate(text):\n",
    "        print(\"......i.......\",i,\"......word.....\",word)\n",
    "        for w in range(3):\n",
    "            # Getting the context that  is ahead by *window* words\n",
    "            if  i + 1 + w < len(text):\n",
    "                word_lists.append([word] + [text[(i + 1 + w)]])\n",
    "            # Getting the context that is behind by *window* words\n",
    "            if i - w - 1 >= 0:\n",
    "                word_lists.append([word] + [text[(i - w - 1)]])\n",
    "                \n",
    "print(\"word list ::::::::::::::: \",word_lists)               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['future',\n",
       " 'king',\n",
       " 'prince',\n",
       " 'daughter',\n",
       " 'princess',\n",
       " 'son',\n",
       " 'prince',\n",
       " 'only',\n",
       " 'man',\n",
       " 'can',\n",
       " 'king',\n",
       " 'only',\n",
       " 'woman',\n",
       " 'can',\n",
       " 'queen',\n",
       " 'princess',\n",
       " 'queen',\n",
       " 'queen',\n",
       " 'king',\n",
       " 'rule',\n",
       " 'realm',\n",
       " 'prince',\n",
       " 'strong',\n",
       " 'man',\n",
       " 'princess',\n",
       " 'beautiful',\n",
       " 'woman',\n",
       " 'royal',\n",
       " 'family',\n",
       " 'king',\n",
       " 'queen',\n",
       " 'their',\n",
       " 'children',\n",
       " 'prince',\n",
       " 'only',\n",
       " 'boy',\n",
       " 'now',\n",
       " 'boy',\n",
       " 'man']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'beautiful': 0, 'boy': 1, 'can': 2, 'children': 3, 'daughter': 4, 'family': 5, 'future': 6, 'king': 7, 'man': 8, 'now': 9, 'only': 10, 'prince': 11, 'princess': 12, 'queen': 13, 'realm': 14, 'royal': 15, 'rule': 16, 'son': 17, 'strong': 18, 'their': 19, 'woman': 20}\n",
      "unique_word_dict :  {'beautiful': 0, 'boy': 1, 'can': 2, 'children': 3, 'daughter': 4, 'family': 5, 'future': 6, 'king': 7, 'man': 8, 'now': 9, 'only': 10, 'prince': 11, 'princess': 12, 'queen': 13, 'realm': 14, 'royal': 15, 'rule': 16, 'son': 17, 'strong': 18, 'their': 19, 'woman': 20}\n"
     ]
    }
   ],
   "source": [
    "unique_word_dict = create_unique_word_dict(all_text)\n",
    "print(\"unique_word_dict : \",unique_word_dict) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Defining the number of features (unique words)\n",
    "n_words = len(unique_word_dict)\n",
    "n_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "words :  ['beautiful', 'boy', 'can', 'children', 'daughter', 'family', 'future', 'king', 'man', 'now', 'only', 'prince', 'princess', 'queen', 'realm', 'royal', 'rule', 'son', 'strong', 'their', 'woman']\n"
     ]
    }
   ],
   "source": [
    "# Getting all the unique words\n",
    "words = list(unique_word_dict.keys())\n",
    "print(\"words : \",words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11it [00:00, 109.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "......i...... 0 .......word_list....... ['future', 'king'] word_list[0] :  future word_list[1] :  king\n",
      "......i...... 1 .......word_list....... ['future', 'prince'] word_list[0] :  future word_list[1] :  prince\n",
      "......i...... 2 .......word_list....... ['king', 'prince'] word_list[0] :  king word_list[1] :  prince\n",
      "......i...... 3 .......word_list....... ['king', 'future'] word_list[0] :  king word_list[1] :  future\n",
      "......i...... 4 .......word_list....... ['prince', 'king'] word_list[0] :  prince word_list[1] :  king\n",
      "......i...... 5 .......word_list....... ['prince', 'future'] word_list[0] :  prince word_list[1] :  future\n",
      "......i...... 6 .......word_list....... ['daughter', 'princess'] word_list[0] :  daughter word_list[1] :  princess\n",
      "......i...... 7 .......word_list....... ['princess', 'daughter'] word_list[0] :  princess word_list[1] :  daughter\n",
      "......i...... 8 .......word_list....... ['son', 'prince'] word_list[0] :  son word_list[1] :  prince\n",
      "......i...... 9 .......word_list....... ['prince', 'son'] word_list[0] :  prince word_list[1] :  son\n",
      "......i...... 10 .......word_list....... ['only', 'man'] word_list[0] :  only word_list[1] :  man\n",
      "......i...... 11 .......word_list....... ['only', 'can'] word_list[0] :  only word_list[1] :  can\n",
      "......i...... 12 .......word_list....... ['only', 'king'] word_list[0] :  only word_list[1] :  king\n",
      "......i...... 13 .......word_list....... ['man', 'can'] word_list[0] :  man word_list[1] :  can\n",
      "......i...... 14 .......word_list....... ['man', 'only'] word_list[0] :  man word_list[1] :  only\n",
      "......i...... 15 .......word_list....... ['man', 'king'] word_list[0] :  man word_list[1] :  king\n",
      "......i...... 16 .......word_list....... ['can', 'king'] word_list[0] :  can word_list[1] :  king\n",
      "......i...... 17 .......word_list....... ['can', 'man'] word_list[0] :  can word_list[1] :  man\n",
      "......i...... 18 .......word_list....... ['can', 'only'] word_list[0] :  can word_list[1] :  only\n",
      "......i...... 19 .......word_list....... ['king', 'can'] word_list[0] :  king word_list[1] :  can\n",
      "......i...... 20 .......word_list....... ['king', 'man'] word_list[0] :  king word_list[1] :  man\n",
      "......i...... 21 .......word_list....... ['king', 'only'] word_list[0] :  king word_list[1] :  only\n",
      "......i...... 22 .......word_list....... ['only', 'woman'] word_list[0] :  only word_list[1] :  woman\n",
      "......i...... 23 .......word_list....... ['only', 'can'] word_list[0] :  only word_list[1] :  can\n",
      "......i...... 24 .......word_list....... ['only', 'queen'] word_list[0] :  only word_list[1] :  queen\n",
      "......i...... 25 .......word_list....... ['woman', 'can'] word_list[0] :  woman word_list[1] :  can\n",
      "......i...... 26 .......word_list....... ['woman', 'only'] word_list[0] :  woman word_list[1] :  only\n",
      "......i...... 27 .......word_list....... ['woman', 'queen'] word_list[0] :  woman word_list[1] :  queen\n",
      "......i...... 28 .......word_list....... ['can', 'queen'] word_list[0] :  can word_list[1] :  queen\n",
      "......i...... 29 .......word_list....... ['can', 'woman'] word_list[0] :  can word_list[1] :  woman\n",
      "......i...... 30 .......word_list....... ['can', 'only'] word_list[0] :  can word_list[1] :  only\n",
      "......i...... 31 .......word_list....... ['queen', 'can'] word_list[0] :  queen word_list[1] :  can\n",
      "......i...... 32 .......word_list....... ['queen', 'woman'] word_list[0] :  queen word_list[1] :  woman\n",
      "......i...... 33 .......word_list....... ['queen', 'only'] word_list[0] :  queen word_list[1] :  only\n",
      "......i...... 34 .......word_list....... ['princess', 'queen'] word_list[0] :  princess word_list[1] :  queen\n",
      "......i...... 35 .......word_list....... ['queen', 'princess'] word_list[0] :  queen word_list[1] :  princess\n",
      "......i...... 36 .......word_list....... ['queen', 'king'] word_list[0] :  queen word_list[1] :  king\n",
      "......i...... 37 .......word_list....... ['queen', 'rule'] word_list[0] :  queen word_list[1] :  rule\n",
      "......i...... 38 .......word_list....... ['queen', 'realm'] word_list[0] :  queen word_list[1] :  realm\n",
      "......i...... 39 .......word_list....... ['king', 'rule'] word_list[0] :  king word_list[1] :  rule\n",
      "......i...... 40 .......word_list....... ['king', 'queen'] word_list[0] :  king word_list[1] :  queen\n",
      "......i...... 41 .......word_list....... ['king', 'realm'] word_list[0] :  king word_list[1] :  realm\n",
      "......i...... 42 .......word_list....... ['rule', 'realm'] word_list[0] :  rule word_list[1] :  realm\n",
      "......i...... 43 .......word_list....... ['rule', 'king'] word_list[0] :  rule word_list[1] :  king\n",
      "......i...... 44 .......word_list....... ['rule', 'queen'] word_list[0] :  rule word_list[1] :  queen\n",
      "......i...... 45 .......word_list....... ['realm', 'rule'] word_list[0] :  realm word_list[1] :  rule\n",
      "......i...... 46 .......word_list....... ['realm', 'king'] word_list[0] :  realm word_list[1] :  king\n",
      "......i...... 47 .......word_list....... ['realm', 'queen'] word_list[0] :  realm word_list[1] :  queen\n",
      "......i...... 48 .......word_list....... ['prince', 'strong'] word_list[0] :  prince word_list[1] :  strong\n",
      "......i...... 49 .......word_list....... ['prince', 'man'] word_list[0] :  prince word_list[1] :  man\n",
      "......i...... 50 .......word_list....... ['strong', 'man'] word_list[0] :  strong word_list[1] :  man\n",
      "......i...... 51 .......word_list....... ['strong', 'prince'] word_list[0] :  strong word_list[1] :  prince\n",
      "......i...... 52 .......word_list....... ['man', 'strong'] word_list[0] :  man word_list[1] :  strong\n",
      "......i...... 53 .......word_list....... ['man', 'prince'] word_list[0] :  man word_list[1] :  prince\n",
      "......i...... 54 .......word_list....... ['princess', 'beautiful'] word_list[0] :  princess word_list[1] :  beautiful\n",
      "......i...... 55 .......word_list....... ['princess', 'woman'] word_list[0] :  princess word_list[1] :  woman\n",
      "......i...... 56 .......word_list....... ['beautiful', 'woman'] word_list[0] :  beautiful word_list[1] :  woman\n",
      "......i...... 57 .......word_list....... ['beautiful', 'princess'] word_list[0] :  beautiful word_list[1] :  princess\n",
      "......i...... 58 .......word_list....... ['woman', 'beautiful'] word_list[0] :  woman word_list[1] :  beautiful\n",
      "......i...... 59 .......word_list....... ['woman', 'princess'] word_list[0] :  woman word_list[1] :  princess\n",
      "......i...... 60 .......word_list....... ['royal', 'family'] word_list[0] :  royal word_list[1] :  family\n",
      "......i...... 61 .......word_list....... ['royal', 'king'] word_list[0] :  royal word_list[1] :  king\n",
      "......i...... 62 .......word_list....... ['royal', 'queen'] word_list[0] :  royal word_list[1] :  queen\n",
      "......i...... 63 .......word_list....... ['family', 'king'] word_list[0] :  family word_list[1] :  king\n",
      "......i...... 64 .......word_list....... ['family', 'royal'] word_list[0] :  family word_list[1] :  royal\n",
      "......i...... 65 .......word_list....... ['family', 'queen'] word_list[0] :  family word_list[1] :  queen\n",
      "......i...... 66 .......word_list....... ['family', 'their'] word_list[0] :  family word_list[1] :  their\n",
      "......i...... 67 .......word_list....... ['king', 'queen'] word_list[0] :  king word_list[1] :  queen\n",
      "......i...... 68 .......word_list....... ['king', 'family'] word_list[0] :  king word_list[1] :  family\n",
      "......i...... 69 .......word_list....... ['king', 'their'] word_list[0] :  king word_list[1] :  their\n",
      "......i...... 70 .......word_list....... ['king', 'royal'] word_list[0] :  king word_list[1] :  royal\n",
      "......i...... 71 .......word_list....... ['king', 'children'] word_list[0] :  king word_list[1] :  children\n",
      "......i...... 72 .......word_list....... ['queen', 'their'] word_list[0] :  queen word_list[1] :  their\n",
      "......i...... 73 .......word_list....... ['queen', 'king'] word_list[0] :  queen word_list[1] :  king\n",
      "......i...... 74 .......word_list....... ['queen', 'children'] word_list[0] :  queen word_list[1] :  children\n",
      "......i...... 75 .......word_list....... ['queen', 'family'] word_list[0] :  queen word_list[1] :  family\n",
      "......i...... 76 .......word_list....... ['queen', 'royal'] word_list[0] :  queen word_list[1] :  royal\n",
      "......i...... 77 .......word_list....... ['their', 'children'] word_list[0] :  their word_list[1] :  children\n",
      "......i...... 78 .......word_list....... ['their', 'queen'] word_list[0] :  their word_list[1] :  queen\n",
      "......i...... 79 .......word_list....... ['their', 'king'] word_list[0] :  their word_list[1] :  king\n",
      "......i...... 80 .......word_list....... ['their', 'family'] word_list[0] :  their word_list[1] :  family\n",
      "......i...... 81 .......word_list....... ['children', 'their'] word_list[0] :  children word_list[1] :  their\n",
      "......i...... 82 .......word_list....... ['children', 'queen'] word_list[0] :  children word_list[1] :  queen\n",
      "......i...... 83 .......word_list....... ['children', 'king'] word_list[0] :  children word_list[1] :  king\n",
      "......i...... 84"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "98it [00:00, 420.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " .......word_list....... ['prince', 'only'] word_list[0] :  prince word_list[1] :  only\n",
      "......i...... 85 .......word_list....... ['prince', 'boy'] word_list[0] :  prince word_list[1] :  boy\n",
      "......i...... 86 .......word_list....... ['prince', 'now'] word_list[0] :  prince word_list[1] :  now\n",
      "......i...... 87 .......word_list....... ['only', 'boy'] word_list[0] :  only word_list[1] :  boy\n",
      "......i...... 88 .......word_list....... ['only', 'prince'] word_list[0] :  only word_list[1] :  prince\n",
      "......i...... 89 .......word_list....... ['only', 'now'] word_list[0] :  only word_list[1] :  now\n",
      "......i...... 90 .......word_list....... ['boy', 'now'] word_list[0] :  boy word_list[1] :  now\n",
      "......i...... 91 .......word_list....... ['boy', 'only'] word_list[0] :  boy word_list[1] :  only\n",
      "......i...... 92 .......word_list....... ['boy', 'prince'] word_list[0] :  boy word_list[1] :  prince\n",
      "......i...... 93 .......word_list....... ['now', 'boy'] word_list[0] :  now word_list[1] :  boy\n",
      "......i...... 94 .......word_list....... ['now', 'only'] word_list[0] :  now word_list[1] :  only\n",
      "......i...... 95 .......word_list....... ['now', 'prince'] word_list[0] :  now word_list[1] :  prince\n",
      "......i...... 96 .......word_list....... ['boy', 'man'] word_list[0] :  boy word_list[1] :  man\n",
      "......i...... 97 .......word_list....... ['man', 'boy'] word_list[0] :  man word_list[1] :  boy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Creating the X  & Y  metrics using one hot encoding\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "X = []\n",
    "Y = []\n",
    "\n",
    "for i , word_list in tqdm(enumerate(word_lists)):\n",
    "    # Getting the indices\n",
    "    \n",
    "    main_word_index = unique_word_dict.get(word_list[0])\n",
    "    context_word_index = unique_word_dict.get(word_list[1])\n",
    "    print(\"......i......\",i,\".......word_list.......\",word_list,\"word_list[0] : \",word_list[0],\"word_list[1] : \",word_list[1])\n",
    "    #Creating the placeholders\n",
    "    X_row = np.zeros(n_words)\n",
    "    Y_row = np.zeros(n_words)\n",
    "    \n",
    "    #One hot encoding the main word\n",
    "    X_row[main_word_index] = 1\n",
    "    \n",
    "    # One hot encoding the Y matrix words\n",
    "    Y_row[context_word_index] = 1\n",
    "    \n",
    "    # Appending to the main matrices\n",
    "    X.append(X_row)\n",
    "    Y.append(Y_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>98 rows Ã— 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     0    1    2    3    4    5    6    7    8    9   ...   11   12   13   14  \\\n",
       "0   0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0   \n",
       "1   0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0   \n",
       "2   0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  ...  0.0  0.0  0.0  0.0   \n",
       "3   0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  ...  0.0  0.0  0.0  0.0   \n",
       "4   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  1.0  0.0  0.0  0.0   \n",
       "..  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...   \n",
       "93  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  ...  0.0  0.0  0.0  0.0   \n",
       "94  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  ...  0.0  0.0  0.0  0.0   \n",
       "95  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  ...  0.0  0.0  0.0  0.0   \n",
       "96  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0   \n",
       "97  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  ...  0.0  0.0  0.0  0.0   \n",
       "\n",
       "     15   16   17   18   19   20  \n",
       "0   0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "1   0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "2   0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "3   0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "4   0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "..  ...  ...  ...  ...  ...  ...  \n",
       "93  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "94  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "95  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "96  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "97  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "\n",
       "[98 rows x 21 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>98 rows Ã— 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     0    1    2    3    4    5    6    7    8    9   ...   11   12   13   14  \\\n",
       "0   0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  ...  0.0  0.0  0.0  0.0   \n",
       "1   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  1.0  0.0  0.0  0.0   \n",
       "2   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  1.0  0.0  0.0  0.0   \n",
       "3   0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0   \n",
       "4   0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  ...  0.0  0.0  0.0  0.0   \n",
       "..  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...   \n",
       "93  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0   \n",
       "94  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0   \n",
       "95  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  1.0  0.0  0.0  0.0   \n",
       "96  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  ...  0.0  0.0  0.0  0.0   \n",
       "97  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0   \n",
       "\n",
       "     15   16   17   18   19   20  \n",
       "0   0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "1   0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "2   0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "3   0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "4   0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "..  ...  ...  ...  ...  ...  ...  \n",
       "93  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "94  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "95  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "96  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "97  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "\n",
       "[98 rows x 21 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the matrices into  a sparse format because the vast majority of the data are 0s\n",
    "from scipy  import sparse\n",
    "X = sparse.csr_matrix(X)\n",
    "Y = sparse.csr_matrix(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<98x21 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 98 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<98x21 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 98 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the size of  the embedding\n",
    "embed_size = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install --upgrade h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install \"keras==2.4.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Defining  the neural network\n",
    "from keras.models import Input,Model\n",
    "from keras.layers import Dense\n",
    "\n",
    "inp = Input(shape=(X.shape[1],))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'input_1:0' shape=(None, 21) dtype=float32>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'dense/Identity:0' shape=(None, 2) dtype=float32>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = Dense(units=embed_size,activation='linear')(inp)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'dense_1/Identity:0' shape=(None, 21) dtype=float32>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = Dense(units=Y.shape[1],activation='softmax')(x)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.engine.training.Model at 0xe8dea6c580>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Model(inputs=inp, outputs=x)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.engine.training.Model at 0xe8dea6c580>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss = 'categorical_crossentropy',optimizer = 'adam')\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import tensorflow as tf'); }\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tensorflow==2.2.0 --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import tensorflow as tf'); }\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "in user code:\n\n    C:\\Users\\Sasmitabhoi\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:571 train_function  *\n        outputs = self.distribute_strategy.run(\n    C:\\Users\\Sasmitabhoi\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:951 run  **\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    C:\\Users\\Sasmitabhoi\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2290 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    C:\\Users\\Sasmitabhoi\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2649 _call_for_each_replica\n        return fn(*args, **kwargs)\n    C:\\Users\\Sasmitabhoi\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:532 train_step  **\n        loss = self.compiled_loss(\n    C:\\Users\\Sasmitabhoi\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\compile_utils.py:205 __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    C:\\Users\\Sasmitabhoi\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\losses.py:143 __call__\n        losses = self.call(y_true, y_pred)\n    C:\\Users\\Sasmitabhoi\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\losses.py:244 call\n        y_pred, y_true = tf_losses_util.squeeze_or_expand_dimensions(\n    C:\\Users\\Sasmitabhoi\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\ops\\losses\\util.py:71 squeeze_or_expand_dimensions\n        y_true, y_pred = confusion_matrix.remove_squeezable_dimensions(\n    C:\\Users\\Sasmitabhoi\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\ops\\confusion_matrix.py:62 remove_squeezable_dimensions\n        labels = ops.convert_to_tensor(labels)\n    C:\\Users\\Sasmitabhoi\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\framework\\ops.py:1341 convert_to_tensor\n        ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\n    C:\\Users\\Sasmitabhoi\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\framework\\constant_op.py:321 _constant_tensor_conversion_function\n        return constant(v, dtype=dtype, name=name)\n    C:\\Users\\Sasmitabhoi\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\framework\\constant_op.py:261 constant\n        return _constant_impl(value, dtype, shape, name, verify_shape=False,\n    C:\\Users\\Sasmitabhoi\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\framework\\constant_op.py:298 _constant_impl\n        tensor_util.make_tensor_proto(\n    C:\\Users\\Sasmitabhoi\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\framework\\tensor_util.py:545 make_tensor_proto\n        raise TypeError(\"Failed to convert object of type %s to Tensor. \"\n\n    TypeError: Failed to convert object of type <class 'tensorflow.python.framework.sparse_tensor.SparseTensor'> to Tensor. Contents: SparseTensor(indices=Tensor(\"DeserializeSparse_1:0\", shape=(None, 2), dtype=int64), values=Tensor(\"DeserializeSparse_1:1\", shape=(None,), dtype=float32), dense_shape=Tensor(\"stack_1:0\", shape=(2,), dtype=int64)). Consider casting elements to a supported type.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-30-6a04ed947c33>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Optimizing the network weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m256\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     64\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 66\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     67\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m     \u001b[1;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m    846\u001b[0m                 batch_size=batch_size):\n\u001b[0;32m    847\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 848\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    849\u001b[0m               \u001b[1;31m# Catch OutOfRangeError for Datasets of unknown size.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    850\u001b[0m               \u001b[1;31m# This blocks until the batch has finished executing.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    578\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    579\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 580\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    581\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    582\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    625\u001b[0m       \u001b[1;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    626\u001b[0m       \u001b[0minitializers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 627\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    628\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    629\u001b[0m       \u001b[1;31m# At this point we know that the initialization is complete (or less\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[1;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[0;32m    503\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_graph_deleter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mFunctionDeleter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lifted_initializer_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    504\u001b[0m     self._concrete_stateful_fn = (\n\u001b[1;32m--> 505\u001b[1;33m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001b[0m\u001b[0;32m    506\u001b[0m             *args, **kwds))\n\u001b[0;32m    507\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2444\u001b[0m       \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2445\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2446\u001b[1;33m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2447\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2448\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   2775\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2776\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2777\u001b[1;33m       \u001b[0mgraph_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2778\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2779\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[1;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m   2655\u001b[0m     \u001b[0marg_names\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbase_arg_names\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mmissing_arg_names\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2656\u001b[0m     graph_function = ConcreteFunction(\n\u001b[1;32m-> 2657\u001b[1;33m         func_graph_module.func_graph_from_py_func(\n\u001b[0m\u001b[0;32m   2658\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2659\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_python_function\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[1;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m    979\u001b[0m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    980\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 981\u001b[1;33m       \u001b[0mfunc_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    982\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    983\u001b[0m       \u001b[1;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m    439\u001b[0m         \u001b[1;31m# __wrapped__ allows AutoGraph to swap in a converted function. We give\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    440\u001b[0m         \u001b[1;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 441\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    442\u001b[0m     \u001b[0mweak_wrapped_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mweakref\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mref\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwrapped_fn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    443\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    966\u001b[0m           \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint:disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    967\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"ag_error_metadata\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 968\u001b[1;33m               \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    969\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    970\u001b[0m               \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: in user code:\n\n    C:\\Users\\Sasmitabhoi\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:571 train_function  *\n        outputs = self.distribute_strategy.run(\n    C:\\Users\\Sasmitabhoi\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:951 run  **\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    C:\\Users\\Sasmitabhoi\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2290 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    C:\\Users\\Sasmitabhoi\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2649 _call_for_each_replica\n        return fn(*args, **kwargs)\n    C:\\Users\\Sasmitabhoi\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:532 train_step  **\n        loss = self.compiled_loss(\n    C:\\Users\\Sasmitabhoi\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\compile_utils.py:205 __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    C:\\Users\\Sasmitabhoi\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\losses.py:143 __call__\n        losses = self.call(y_true, y_pred)\n    C:\\Users\\Sasmitabhoi\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\losses.py:244 call\n        y_pred, y_true = tf_losses_util.squeeze_or_expand_dimensions(\n    C:\\Users\\Sasmitabhoi\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\ops\\losses\\util.py:71 squeeze_or_expand_dimensions\n        y_true, y_pred = confusion_matrix.remove_squeezable_dimensions(\n    C:\\Users\\Sasmitabhoi\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\ops\\confusion_matrix.py:62 remove_squeezable_dimensions\n        labels = ops.convert_to_tensor(labels)\n    C:\\Users\\Sasmitabhoi\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\framework\\ops.py:1341 convert_to_tensor\n        ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\n    C:\\Users\\Sasmitabhoi\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\framework\\constant_op.py:321 _constant_tensor_conversion_function\n        return constant(v, dtype=dtype, name=name)\n    C:\\Users\\Sasmitabhoi\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\framework\\constant_op.py:261 constant\n        return _constant_impl(value, dtype, shape, name, verify_shape=False,\n    C:\\Users\\Sasmitabhoi\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\framework\\constant_op.py:298 _constant_impl\n        tensor_util.make_tensor_proto(\n    C:\\Users\\Sasmitabhoi\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\framework\\tensor_util.py:545 make_tensor_proto\n        raise TypeError(\"Failed to convert object of type %s to Tensor. \"\n\n    TypeError: Failed to convert object of type <class 'tensorflow.python.framework.sparse_tensor.SparseTensor'> to Tensor. Contents: SparseTensor(indices=Tensor(\"DeserializeSparse_1:0\", shape=(None, 2), dtype=int64), values=Tensor(\"DeserializeSparse_1:1\", shape=(None,), dtype=float32), dense_shape=Tensor(\"stack_1:0\", shape=(2,), dtype=int64)). Consider casting elements to a supported type.\n"
     ]
    }
   ],
   "source": [
    "# Optimizing the network weights\n",
    "model.fit(x=X,y=Y,batch_size=256,epochs=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[-0.48525923,  0.48989838],\n",
       "        [-0.40105975,  0.06084579],\n",
       "        [-0.18636236, -0.43697688],\n",
       "        [ 0.19304371,  0.03514731],\n",
       "        [ 0.4949897 ,  0.36388546],\n",
       "        [ 0.5055823 ,  0.3396532 ],\n",
       "        [ 0.42373186, -0.22213167],\n",
       "        [-0.3171456 , -0.13002518],\n",
       "        [ 0.28622043, -0.17907482],\n",
       "        [ 0.08363825, -0.43413982],\n",
       "        [ 0.3200227 ,  0.16866517],\n",
       "        [-0.35240686, -0.3959074 ],\n",
       "        [-0.14400038,  0.34702116],\n",
       "        [-0.22663495, -0.46553904],\n",
       "        [-0.4799234 ,  0.50263435],\n",
       "        [-0.31999823,  0.4542346 ],\n",
       "        [-0.09290153, -0.15690821],\n",
       "        [ 0.410519  ,  0.10641861],\n",
       "        [ 0.4113769 ,  0.40868533],\n",
       "        [ 0.23212487, -0.23161218],\n",
       "        [ 0.4923479 ,  0.17614555]], dtype=float32),\n",
       " array([0., 0.], dtype=float32),\n",
       " array([[ 0.1885621 , -0.03902417,  0.5025458 ,  0.3273633 , -0.42994267,\n",
       "         -0.19488099, -0.10760868, -0.14810571, -0.22260839, -0.28364187,\n",
       "          0.44162548,  0.3404408 ,  0.28472197, -0.24765837, -0.05950752,\n",
       "         -0.22959259, -0.4598118 , -0.3152592 ,  0.23235369, -0.37954292,\n",
       "         -0.17077929],\n",
       "        [-0.00481635,  0.42334306,  0.45470887, -0.07555372, -0.06951326,\n",
       "          0.06557673,  0.47588176, -0.2195805 , -0.4700041 ,  0.05812538,\n",
       "         -0.40822452,  0.18352932, -0.4747932 , -0.4047228 ,  0.08903813,\n",
       "         -0.23235708, -0.08716124, -0.4489181 , -0.37175018,  0.5067306 ,\n",
       "          0.29939395]], dtype=float32),\n",
       " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0.], dtype=float32)]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = model.get_weights()\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtaining the weights from the neural network\n",
    "# These are the so called word embeddings\n",
    "\n",
    "# The input layer\n",
    "weights = model.get_weights()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.48525923,  0.48989838],\n",
       "       [-0.40105975,  0.06084579],\n",
       "       [-0.18636236, -0.43697688],\n",
       "       [ 0.19304371,  0.03514731],\n",
       "       [ 0.4949897 ,  0.36388546],\n",
       "       [ 0.5055823 ,  0.3396532 ],\n",
       "       [ 0.42373186, -0.22213167],\n",
       "       [-0.3171456 , -0.13002518],\n",
       "       [ 0.28622043, -0.17907482],\n",
       "       [ 0.08363825, -0.43413982],\n",
       "       [ 0.3200227 ,  0.16866517],\n",
       "       [-0.35240686, -0.3959074 ],\n",
       "       [-0.14400038,  0.34702116],\n",
       "       [-0.22663495, -0.46553904],\n",
       "       [-0.4799234 ,  0.50263435],\n",
       "       [-0.31999823,  0.4542346 ],\n",
       "       [-0.09290153, -0.15690821],\n",
       "       [ 0.410519  ,  0.10641861],\n",
       "       [ 0.4113769 ,  0.40868533],\n",
       "       [ 0.23212487, -0.23161218],\n",
       "       [ 0.4923479 ,  0.17614555]], dtype=float32)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'beautiful': array([-0.48525923,  0.48989838], dtype=float32),\n",
       " 'boy': array([-0.40105975,  0.06084579], dtype=float32),\n",
       " 'can': array([-0.18636236, -0.43697688], dtype=float32),\n",
       " 'children': array([0.19304371, 0.03514731], dtype=float32),\n",
       " 'daughter': array([0.4949897 , 0.36388546], dtype=float32),\n",
       " 'family': array([0.5055823, 0.3396532], dtype=float32),\n",
       " 'future': array([ 0.42373186, -0.22213167], dtype=float32),\n",
       " 'king': array([-0.3171456 , -0.13002518], dtype=float32),\n",
       " 'man': array([ 0.28622043, -0.17907482], dtype=float32),\n",
       " 'now': array([ 0.08363825, -0.43413982], dtype=float32),\n",
       " 'only': array([0.3200227 , 0.16866517], dtype=float32),\n",
       " 'prince': array([-0.35240686, -0.3959074 ], dtype=float32),\n",
       " 'princess': array([-0.14400038,  0.34702116], dtype=float32),\n",
       " 'queen': array([-0.22663495, -0.46553904], dtype=float32),\n",
       " 'realm': array([-0.4799234 ,  0.50263435], dtype=float32),\n",
       " 'royal': array([-0.31999823,  0.4542346 ], dtype=float32),\n",
       " 'rule': array([-0.09290153, -0.15690821], dtype=float32),\n",
       " 'son': array([0.410519  , 0.10641861], dtype=float32),\n",
       " 'strong': array([0.4113769 , 0.40868533], dtype=float32),\n",
       " 'their': array([ 0.23212487, -0.23161218], dtype=float32),\n",
       " 'woman': array([0.4923479 , 0.17614555], dtype=float32)}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating a dictionary to store the embeddings in . They key is a unique word and  \n",
    "# the value is the numeric vector\n",
    "embedding_dict = {}\n",
    "for word in words:\n",
    "    embedding_dict.update({\n",
    "        word: weights[unique_word_dict.get(word)]\n",
    "    })\n",
    "    \n",
    "embedding_dict    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'beautiful': 0,\n",
       " 'boy': 1,\n",
       " 'can': 2,\n",
       " 'children': 3,\n",
       " 'daughter': 4,\n",
       " 'family': 5,\n",
       " 'future': 6,\n",
       " 'king': 7,\n",
       " 'man': 8,\n",
       " 'now': 9,\n",
       " 'only': 10,\n",
       " 'prince': 11,\n",
       " 'princess': 12,\n",
       " 'queen': 13,\n",
       " 'realm': 14,\n",
       " 'royal': 15,\n",
       " 'rule': 16,\n",
       " 'son': 17,\n",
       " 'strong': 18,\n",
       " 'their': 19,\n",
       " 'woman': 20}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_word_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Bad key \"text.kerning_factor\" on line 4 in\n",
      "C:\\Users\\Sasmitabhoi\\anaconda3\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test_patch.mplstyle.\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "https://github.com/matplotlib/matplotlib/blob/v3.1.3/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import tensorflow as tf\\nimport matplotlib.pyplot as plt'); }\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import tensorflow as tf\\nimport matplotlib.pyplot as plt'); }\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import tensorflow as tf\\nimport matplotlib.pyplot as plt'); }\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import tensorflow as tf\\nimport matplotlib.pyplot as plt'); }\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import tensorflow as tf\\nimport matplotlib.pyplot as plt'); }\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import tensorflow as tf\\nimport matplotlib.pyplot as plt'); }\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import tensorflow as tf\\nimport matplotlib.pyplot as plt'); }\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import tensorflow as tf\\nimport matplotlib.pyplot as plt'); }\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import tensorflow as tf\\nimport matplotlib.pyplot as plt'); }\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import tensorflow as tf\\nimport matplotlib.pyplot as plt'); }\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import tensorflow as tf\\nimport matplotlib.pyplot as plt'); }\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import tensorflow as tf\\nimport matplotlib.pyplot as plt'); }\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import tensorflow as tf\\nimport matplotlib.pyplot as plt'); }\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import tensorflow as tf\\nimport matplotlib.pyplot as plt'); }\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import tensorflow as tf\\nimport matplotlib.pyplot as plt'); }\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import tensorflow as tf\\nimport matplotlib.pyplot as plt'); }\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import tensorflow as tf\\nimport matplotlib.pyplot as plt'); }\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import tensorflow as tf\\nimport matplotlib.pyplot as plt'); }\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import tensorflow as tf\\nimport matplotlib.pyplot as plt'); }\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import tensorflow as tf\\nimport matplotlib.pyplot as plt'); }\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import tensorflow as tf\\nimport matplotlib.pyplot as plt'); }\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import tensorflow as tf\\nimport matplotlib.pyplot as plt'); }\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import tensorflow as tf\\nimport matplotlib.pyplot as plt'); }\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import tensorflow as tf\\nimport matplotlib.pyplot as plt'); }\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import tensorflow as tf\\nimport matplotlib.pyplot as plt'); }\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import tensorflow as tf\\nimport matplotlib.pyplot as plt'); }\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import tensorflow as tf\\nimport matplotlib.pyplot as plt'); }\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import tensorflow as tf\\nimport matplotlib.pyplot as plt'); }\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import tensorflow as tf\\nimport matplotlib.pyplot as plt'); }\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import tensorflow as tf\\nimport matplotlib.pyplot as plt'); }\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import tensorflow as tf\\nimport matplotlib.pyplot as plt'); }\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import tensorflow as tf\\nimport matplotlib.pyplot as plt'); }\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import tensorflow as tf\\nimport matplotlib.pyplot as plt'); }\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import tensorflow as tf\\nimport matplotlib.pyplot as plt'); }\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import tensorflow as tf\\nimport matplotlib.pyplot as plt'); }\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import tensorflow as tf\\nimport matplotlib.pyplot as plt'); }\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import tensorflow as tf\\nimport matplotlib.pyplot as plt'); }\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import tensorflow as tf\\nimport matplotlib.pyplot as plt'); }\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import tensorflow as tf\\nimport matplotlib.pyplot as plt'); }\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import tensorflow as tf\\nimport matplotlib.pyplot as plt'); }\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import tensorflow as tf\\nimport matplotlib.pyplot as plt'); }\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import tensorflow as tf\\nimport matplotlib.pyplot as plt'); }\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import tensorflow as tf\\nimport matplotlib.pyplot as plt'); }\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotting the embeddings\n",
    "plt.figure(figsize=(10,10))\n",
    "for word in list(unique_word_dict.keys()):\n",
    "    coord = embedding_dict.get(word)\n",
    "    plt.scatter(coord[0],coord[1])\n",
    "    plt.annotate(word,(coord[0],coord[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What is the dimension here : 98 * 20 ( 98 - rows , 20 - cols )\n",
    "# Here 98*20 is the input and 2D,3D - output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import tensorflow as tf\\nimport os\\nimport matplotlib.pyplot as plt'); }\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import tensorflow as tf\\nimport os\\nimport matplotlib.pyplot as plt'); }\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import tensorflow as tf\\nimport os\\nimport matplotlib.pyplot as plt'); }\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Saving the embedding vector to a txt file\n",
    "try:\n",
    "    os.mkdir(f'{os.getcwd()}\\\\output')\n",
    "except Exception as e:\n",
    "    print(f'Cannot create output folder:{e}')\n",
    "\n",
    "with open(f'{os.getcwd()}\\\\output\\\\embedding.txt','w') as f:\n",
    "    for key, value  in embedding_dict.items():\n",
    "        try:\n",
    "            f.write(f'{key}: {value}\\n')\n",
    "        except Exception as e:\n",
    "            print(f'Cannot write word {key} to dict: {e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
